<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description" content="EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos">
    <meta name="keywords" content="Hand-Object Interaction, Egocentric Vision, 3D Reconstruction, Computer Vision">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="css/custom.css">
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos</h1>
                        
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><a href="https://frank-f2022.github.io/" target="_blank">Hongming Fu</a><sup>1</sup></span>
                            <span class="author-block"><a href="https://wenjiawang0312.github.io/" target="_blank">Wenjia Wang</a><sup>2†</sup></span>
                            <span class="author-block">Xiaozhen Qiao<sup>3</sup></span>
                            <span class="author-block"><a href="https://shuoyang-1998.github.io/" target="_blank">Shuo Yang</a><sup>4</sup></span>
                            <span class="author-block"><a href="https://scholar.google.com/citations?user=k2SF4M0AAAAJ&hl=en" target="_blank">Zheng Liu</a><sup>5</sup></span>
                            <span class="author-block"><a href="https://www.bozhao.me/" target="_blank">Bo Zhao</a><sup>1‡</sup></span>
                        </div>

                        <div class="is-size-6 publication-authors">
                            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University</span>
                            <span class="author-block"><sup>2</sup>The University of Hong Kong</span>
                        </div>
                        <div class="is-size-6 publication-authors">
                            <span class="author-block"><sup>3</sup>University of Science and Technology of China</span>
                            <span class="author-block"><sup>4</sup>Harbin Institute of Technology (Shenzhen)</span>
                        </div>
                        <div class="is-size-6 publication-authors">
                            <span class="author-block"><sup>5</sup>Beijing Academy of Artificial Intelligence</span>
                        </div>
                        <div class="is-size-7 publication-authors" style="margin-top: 10px;">
                            <span class="author-block"><sup>†</sup>Project lead</span>
                            <span class="author-block"><sup>‡</sup>Corresponding author</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2601.01050" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon"><i class="ai ai-arxiv"></i></span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="#" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon"><i class="fas fa-file-pdf"></i></span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="#" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon"><i class="fab fa-github"></i></span>
                                        <span>Code (Comming soon)</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Teaser Video/Image -->
    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body" style="text-align: center;">
                <img src="assets/images/teaser-5.png" alt="EgoGrasp Teaser" style="width: 50%; height: auto;">
            </div>
        </div>
    </section>

    <!-- Abstract -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>We propose <strong>EgoGrasp</strong>, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Method Pipeline -->
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">Method Overview</h2>
            <div class="hero-body">
                <img src="assets/images/pipeline.png" alt="EgoGrasp Pipeline" style="width: 100%; height: auto;">
            </div>
        </div>
    </section>

    <!-- Video Results -->
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">Video Demonstrations</h2>
            <div class="columns is-multiline">
                <div class="column is-one-third">
                    <div class="video-item">
                        <div class="video-placeholder-small">
                            <p>[Demo 1]</p>
                        </div>
                        <p class="has-text-centered">[Demo 1 caption]</p>
                    </div>
                </div>
                <div class="column is-one-third">
                    <div class="video-item">
                        <div class="video-placeholder-small">
                            <p>[Demo 2]</p>
                        </div>
                        <p class="has-text-centered">[Demo 2 caption]</p>
                    </div>
                </div>
                <div class="column is-one-third">
                    <div class="video-item">
                        <div class="video-placeholder-small">
                            <p>[Demo 3]</p>
                        </div>
                        <p class="has-text-centered">[Demo 3 caption]</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Qualitative Results -->
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
            <div class="columns is-multiline">
                <div class="column is-half">
                    <div class="image-placeholder">
                        <p>[Result Image 1]</p>
                    </div>
                    <p class="has-text-centered">[Caption 1]</p>
                </div>
                <div class="column is-half">
                    <div class="image-placeholder">
                        <p>[Result Image 2]</p>
                    </div>
                    <p class="has-text-centered">[Caption 2]</p>
                </div>
                <div class="column is-half">
                    <div class="image-placeholder">
                        <p>[Result Image 3]</p>
                    </div>
                    <p class="has-text-centered">[Caption 3]</p>
                </div>
                <div class="column is-half">
                    <div class="image-placeholder">
                        <p>[Result Image 4]</p>
                    </div>
                    <p class="has-text-centered">[Caption 4]</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title is-3 has-text-centered">BibTeX</h2>
            <pre><code>@misc{fu2026egograspworldspacehandobjectinteraction,
      title={EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos}, 
      author={Hongming Fu and Wenjia Wang and Xiaozhen Qiao and Shuo Yang and Zheng Liu and Bo Zhao},
      year={2026},
      eprint={2601.01050},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2601.01050}, 
}</code></pre>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <p>
                    This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                </p>
                <p>
                    Website template based on <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                </p>
            </div>
        </div>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const videos = document.querySelectorAll('video');
            videos.forEach(video => {
                const volume = video.getAttribute('data-volume');
                if (volume) video.volume = parseFloat(volume);
            });
        });
    </script>
</body>
</html>
